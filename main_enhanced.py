# üè¶ Financial AI Dashboard - –ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ RSS –∫–∞–Ω–∞–ª–∞–º–∏

from fastapi import FastAPI, HTTPException, Depends, status, BackgroundTasks, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse, JSONResponse
from sqlalchemy import create_engine, Column, String, Boolean, DateTime, Text, JSON, Integer, Float, func
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.dialects.postgresql import UUID
from passlib.context import CryptContext
from datetime import datetime, timedelta, timezone
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, EmailStr, HttpUrl
import jwt
import uuid
import os
import httpx
import asyncio
import json
import logging
from pathlib import Path
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import schedule
import threading
import time
import random
from jinja2 import Template
import xml.etree.ElementTree as ET
import feedparser
from openai import OpenAI
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
SECRET_KEY = os.getenv("SECRET_KEY", "financial-ai-super-secret-key-for-development")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 1440  # 24 —á–∞—Å–∞

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://finai_user:finai_password@localhost/finai_db")
engine = create_engine(DATABASE_URL, echo=False)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
NBK_API_URL = "https://www.nationalbank.kz/rss/get_rates.cfm"  # –ù–ë –†–ö API
NEWS_API_KEY = os.getenv("NEWS_API_KEY", "")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI
if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)
else:
    client = None
    logger.warning("OpenAI API key not configured")

# –•–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–æ–ª–µ–π
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
security = HTTPBearer()

# FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="Financial AI Dashboard - Enhanced with Custom RSS",
    description="AI-powered Corporate Liquidity Management with Custom RSS Feeds",
    version="2.1.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# CORS –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –°—Ç–∞—Ç–∏—á–Ω—ã–µ —Ñ–∞–π–ª—ã
app.mount("/static", StaticFiles(directory="static"), name="static")

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
Path("logs").mkdir(exist_ok=True)
Path("static/reports").mkdir(exist_ok=True, parents=True)
Path("static/uploads").mkdir(exist_ok=True, parents=True)

# =============================================================================
# –ú–æ–¥–µ–ª–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
# =============================================================================

class User(Base):
    __tablename__ = "users"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    email = Column(String, unique=True, index=True, nullable=False)
    name = Column(String, nullable=False)
    hashed_password = Column(String, nullable=False)
    role = Column(String, default="user")
    company = Column(String)
    phone = Column(String)
    subscription_plan = Column(String, default="professional")
    is_active = Column(Boolean, default=True)
    timezone = Column(String, default="Asia/Almaty")
    language = Column(String, default="ru")
    created_at = Column(DateTime, default=datetime.utcnow)
    last_login = Column(DateTime)

class BankAccount(Base):
    __tablename__ = "bank_accounts"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), nullable=False)
    name = Column(String, nullable=False)
    bank = Column(String, nullable=False)
    balance = Column(Float, default=0.0)
    currency = Column(String, default="KZT")
    account_type = Column(String, default="operational")
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class ExchangeRate(Base):
    __tablename__ = "exchange_rates"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    from_currency = Column(String, nullable=False)
    to_currency = Column(String, nullable=False)
    rate = Column(Float, nullable=False)
    date = Column(DateTime, nullable=False)
    source = Column(String, default="NBK")
    created_at = Column(DateTime, default=datetime.utcnow)

class NewsArticle(Base):
    __tablename__ = "news_articles"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    title = Column(String, nullable=False)
    content = Column(Text)
    summary = Column(Text)
    url = Column(String)
    published_at = Column(DateTime)
    source = Column(String)
    category = Column(String, default="financial")
    sentiment = Column(String)  # positive, negative, neutral
    risk_level = Column(String)  # low, medium, high
    relevance_score = Column(Float, default=0.5)  # AI –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    ai_tags = Column(JSON)  # AI —Ç–µ–≥–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
    created_at = Column(DateTime, default=datetime.utcnow)

# –ù–û–í–ê–Ø –ú–û–î–ï–õ–¨: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ RSS –∫–∞–Ω–∞–ª—ã
class UserRSSFeed(Base):
    __tablename__ = "user_rss_feeds"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), nullable=False)
    name = Column(String, nullable=False)  # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞
    url = Column(String, nullable=False)   # RSS URL
    description = Column(Text)             # –û–ø–∏—Å–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞
    category = Column(String, default="financial")  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
    priority = Column(Integer, default=1)  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (1-5)
    is_active = Column(Boolean, default=True)
    auto_analysis = Column(Boolean, default=True)  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π AI –∞–Ω–∞–ª–∏–∑
    keywords = Column(JSON)  # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    last_fetched = Column(DateTime)
    fetch_frequency = Column(String, default="hourly")  # hourly, daily, weekly
    error_count = Column(Integer, default=0)
    last_error = Column(Text)
    articles_count = Column(Integer, default=0)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

# –ù–û–í–ê–Ø –ú–û–î–ï–õ–¨: –ê–Ω–∞–ª–∏–∑ RSS –∫–æ–Ω—Ç–µ–Ω—Ç–∞
class RSSContentAnalysis(Base):
    __tablename__ = "rss_content_analysis"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), nullable=False)
    rss_feed_id = Column(UUID(as_uuid=True), nullable=False)
    article_id = Column(UUID(as_uuid=True))  # –°–≤—è–∑—å —Å news_articles
    analysis_date = Column(DateTime, default=datetime.utcnow)
    
    # AI –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    financial_relevance = Column(Float)  # 0-1 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤
    risk_indicators = Column(JSON)       # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∏—Å–∫–æ–≤
    sentiment_score = Column(Float)      # -1 to 1 (–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π - –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π)
    key_topics = Column(JSON)           # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã
    market_impact = Column(String)      # low, medium, high
    recommendations = Column(Text)      # AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞
    processing_time_ms = Column(Integer)
    ai_confidence = Column(Float)       # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å AI –≤ –∞–Ω–∞–ª–∏–∑–µ
    created_at = Column(DateTime, default=datetime.utcnow)

class AIConsultation(Base):
    __tablename__ = "ai_consultations"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True))
    question = Column(Text, nullable=False)
    response = Column(Text, nullable=False)
    context_data = Column(JSON)
    rss_sources_used = Column(JSON)  # –ù–û–í–û–ï: RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –≤ –æ—Ç–≤–µ—Ç–µ
    session_id = Column(String)
    source = Column(String, default="web")  # web, n8n, telegram
    created_at = Column(DateTime, default=datetime.utcnow)

class Report(Base):
    __tablename__ = "reports"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True))
    name = Column(String, nullable=False)
    report_type = Column(String, nullable=False)
    content = Column(Text)
    file_path = Column(String)
    rss_feeds_included = Column(JSON)  # –ù–û–í–û–ï: RSS –∫–∞–Ω–∞–ª—ã –≤–∫–ª—é—á–µ–Ω–Ω—ã–µ –≤ –æ—Ç—á–µ—Ç
    generated_at = Column(DateTime, default=datetime.utcnow)
    
class WebhookLog(Base):
    __tablename__ = "webhook_logs"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    source = Column(String, nullable=False)  # n8n, telegram, etc
    event_type = Column(String)
    payload = Column(JSON)
    response = Column(JSON)
    processed_at = Column(DateTime, default=datetime.utcnow)

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
Base.metadata.create_all(bind=engine)

# =============================================================================
# Pydantic –º–æ–¥–µ–ª–∏
# =============================================================================

class UserCreate(BaseModel):
    email: EmailStr
    name: str
    password: str
    company: Optional[str] = None

class UserLogin(BaseModel):
    email: EmailStr
    password: str

class AIQuestionRequest(BaseModel):
    question: str
    context: Optional[Dict[str, Any]] = {}
    source: Optional[str] = "web"
    include_rss: Optional[bool] = True  # –ù–û–í–û–ï: –≤–∫–ª—é—á–∞—Ç—å RSS –≤ –∞–Ω–∞–ª–∏–∑

class N8NWebhookPayload(BaseModel):
    message: str
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    webhook_url: Optional[str] = None

# –ù–û–í–´–ï –ú–û–î–ï–õ–ò –¥–ª—è RSS
class RSSFeedCreate(BaseModel):
    name: str
    url: HttpUrl
    description: Optional[str] = ""
    category: Optional[str] = "financial"
    priority: Optional[int] = 1
    auto_analysis: Optional[bool] = True
    keywords: Optional[List[str]] = []
    fetch_frequency: Optional[str] = "hourly"

class RSSFeedUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    category: Optional[str] = None
    priority: Optional[int] = None
    auto_analysis: Optional[bool] = None
    keywords: Optional[List[str]] = None
    fetch_frequency: Optional[str] = None
    is_active: Optional[bool] = None

class RSSFeedResponse(BaseModel):
    id: str
    name: str
    url: str
    description: str
    category: str
    priority: int
    is_active: bool
    auto_analysis: bool
    keywords: List[str]
    last_fetched: Optional[datetime]
    fetch_frequency: str
    error_count: int
    articles_count: int
    created_at: datetime

# =============================================================================
# –£—Ç–∏–ª–∏—Ç—ã
# =============================================================================

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password):
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security), db: Session = Depends(get_db)):
    token = credentials.credentials
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        email: str = payload.get("sub")
        if email is None:
            raise HTTPException(status_code=401, detail="Invalid token")
    except jwt.PyJWTError:
        raise HTTPException(status_code=401, detail="Invalid token")
    
    user = db.query(User).filter(User.email == email).first()
    if user is None:
        raise HTTPException(status_code=401, detail="User not found")
    return user

def validate_rss_url(url: str) -> bool:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è RSS URL"""
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å RSS
        response = requests.head(url, timeout=10)
        return response.status_code == 200
    except:
        return False

# =============================================================================
# –°–µ—Ä–≤–∏—Å—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ)
# =============================================================================

class NBKExchangeRateService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫—É—Ä—Å–æ–≤ –≤–∞–ª—é—Ç –∏–∑ –ù–ë –†–ö"""
    
    @staticmethod
    async def fetch_exchange_rates():
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—É—Ä—Å–æ–≤ –≤–∞–ª—é—Ç –∏–∑ –ù–ë –†–ö"""
        try:
            logger.info("Fetching exchange rates from NBK...")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(NBK_API_URL)
                response.raise_for_status()
                
            # –ü–∞—Ä—Å–∏–Ω–≥ XML –æ—Ç–≤–µ—Ç–∞ –æ—Ç –ù–ë –†–ö
            root = ET.fromstring(response.content)
            rates = []
            
            # –ù–ë –†–ö –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ XML
            for item in root.findall('.//item'):
                try:
                    title = item.find('title').text if item.find('title') is not None else ""
                    description = item.find('description').text if item.find('description') is not None else ""
                    pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—É—Ä—Å–∞ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
                    if "USD" in title:
                        rate_text = description.split()[-1]
                        rate = float(rate_text.replace(',', '.'))
                        rates.append({
                            'from_currency': 'USD',
                            'to_currency': 'KZT',
                            'rate': rate,
                            'date': datetime.now(timezone.utc),
                            'source': 'NBK'
                        })
                    elif "EUR" in title:
                        rate_text = description.split()[-1]
                        rate = float(rate_text.replace(',', '.'))
                        rates.append({
                            'from_currency': 'EUR',
                            'to_currency': 'KZT',
                            'rate': rate,
                            'date': datetime.now(timezone.utc),
                            'source': 'NBK'
                        })
                    elif "RUB" in title:
                        rate_text = description.split()[-1]
                        rate = float(rate_text.replace(',', '.'))
                        rates.append({
                            'from_currency': 'RUB',
                            'to_currency': 'KZT',
                            'rate': rate,
                            'date': datetime.now(timezone.utc),
                            'source': 'NBK'
                        })
                except (ValueError, AttributeError) as e:
                    logger.warning(f"Error parsing rate item: {e}")
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–∞—Ä—Å–∏—Ç—å XML, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback API –∏–ª–∏ —Å—Ç–∞—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if not rates:
                logger.warning("Failed to parse NBK XML, using fallback rates")
                rates = [
                    {'from_currency': 'USD', 'to_currency': 'KZT', 'rate': 480.5, 'date': datetime.now(timezone.utc), 'source': 'NBK_FALLBACK'},
                    {'from_currency': 'EUR', 'to_currency': 'KZT', 'rate': 520.3, 'date': datetime.now(timezone.utc), 'source': 'NBK_FALLBACK'},
                    {'from_currency': 'RUB', 'to_currency': 'KZT', 'rate': 5.2, 'date': datetime.now(timezone.utc), 'source': 'NBK_FALLBACK'},
                ]
            
            logger.info(f"Fetched {len(rates)} exchange rates from NBK")
            return rates
            
        except Exception as e:
            logger.error(f"Error fetching NBK exchange rates: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫—É—Ä—Å—ã –∏–ª–∏ —Å—Ç–∞—Ç–∏—á–Ω—ã–µ
            return [
                {'from_currency': 'USD', 'to_currency': 'KZT', 'rate': 480.5, 'date': datetime.now(timezone.utc), 'source': 'FALLBACK'},
                {'from_currency': 'EUR', 'to_currency': 'KZT', 'rate': 520.3, 'date': datetime.now(timezone.utc), 'source': 'FALLBACK'},
                {'from_currency': 'RUB', 'to_currency': 'KZT', 'rate': 5.2, 'date': datetime.now(timezone.utc), 'source': 'FALLBACK'},
            ]

class EnhancedNewsService:
    """–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ RSS"""
    
    @staticmethod
    async def fetch_default_news():
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –±–∞–∑–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        try:
            logger.info("Fetching default financial news...")
            
            default_sources = [
                {
                    'url': 'https://feedpress.me/tengrinews-economy',
                    'source': 'Tengrinews',
                    'category': 'economy'
                },
                {
                    'url': 'https://forbes.kz/rss/',
                    'source': 'Forbes Kazakhstan',
                    'category': 'financial'
                },
                {
                    'url': 'https://kursiv.kz/feed/',
                    'source': 'Kursiv',
                    'category': 'business'
                }
            ]
            
            all_articles = []
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS –ª–µ–Ω—Ç
            for source in default_sources:
                try:
                    async with httpx.AsyncClient(timeout=20.0) as client:
                        response = await client.get(source['url'])
                        response.raise_for_status()
                    
                    feed = feedparser.parse(response.content)
                    
                    for entry in feed.entries[:5]:  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                        article = {
                            'title': entry.get('title', ''),
                            'content': entry.get('summary', ''),
                            'url': entry.get('link', ''),
                            'published_at': datetime.now(timezone.utc),
                            'source': source['source'],
                            'category': source['category']
                        }
                        all_articles.append(article)
                        
                except Exception as e:
                    logger.warning(f"Error fetching from {source['source']}: {e}")
                    continue
            
            logger.info(f"Fetched {len(all_articles)} default news articles")
            return all_articles
            
        except Exception as e:
            logger.error(f"Error fetching default news: {e}")
            return []

    @staticmethod
    async def fetch_user_rss_feeds(user_id: str, db: Session):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö RSS –∫–∞–Ω–∞–ª–æ–≤"""
        try:
            logger.info(f"Fetching user RSS feeds for user {user_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ RSS –∫–∞–Ω–∞–ª—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_feeds = db.query(UserRSSFeed).filter(
                UserRSSFeed.user_id == user_id,
                UserRSSFeed.is_active == True
            ).all()
            
            if not user_feeds:
                logger.info(f"No active RSS feeds for user {user_id}")
                return []
            
            all_articles = []
            
            for feed in user_feeds:
                try:
                    logger.info(f"Processing RSS feed: {feed.name} - {feed.url}")
                    
                    async with httpx.AsyncClient(timeout=20.0) as client:
                        response = await client.get(str(feed.url))
                        response.raise_for_status()
                    
                    parsed_feed = feedparser.parse(response.content)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
                    max_articles = min(feed.priority * 3, 15)  # 3-15 —Å—Ç–∞—Ç–µ–π
                    
                    for entry in parsed_feed.entries[:max_articles]:
                        article = {
                            'title': entry.get('title', ''),
                            'content': entry.get('summary', '') or entry.get('description', ''),
                            'url': entry.get('link', ''),
                            'published_at': datetime.now(timezone.utc),
                            'source': feed.name,
                            'category': feed.category,
                            'rss_feed_id': str(feed.id),
                            'user_id': user_id
                        }
                        
                        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                        if feed.keywords:
                            title_lower = article['title'].lower()
                            content_lower = article['content'].lower()
                            
                            if any(keyword.lower() in title_lower or keyword.lower() in content_lower 
                                   for keyword in feed.keywords):
                                article['relevance_score'] = 0.8  # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                                all_articles.append(article)
                            elif feed.priority >= 4:  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - –±–µ—Ä–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏
                                article['relevance_score'] = 0.6
                                all_articles.append(article)
                        else:
                            article['relevance_score'] = 0.7
                            all_articles.append(article)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–∞
                    feed.last_fetched = datetime.utcnow()
                    feed.error_count = 0
                    feed.articles_count += len(parsed_feed.entries[:max_articles])
                    feed.last_error = None
                    
                except Exception as e:
                    logger.error(f"Error fetching RSS feed {feed.name}: {e}")
                    feed.error_count += 1
                    feed.last_error = str(e)
                    continue
            
            db.commit()
            logger.info(f"Fetched {len(all_articles)} articles from user RSS feeds")
            return all_articles
            
        except Exception as e:
            logger.error(f"Error fetching user RSS feeds: {e}")
            return []

class EnhancedAIAnalysisService:
    """–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π AI —Å–µ—Ä–≤–∏—Å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö RSS"""
    
    @staticmethod
    async def analyze_rss_content(article_data: Dict, rss_feed_id: str, user_id: str, db: Session):
        """AI –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ RSS –∫–∞–Ω–∞–ª–∞"""
        try:
            if not client:
                logger.warning("OpenAI client not available for RSS analysis")
                return None
            
            title = article_data.get('title', '')
            content = article_data.get('content', '')
            
            prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â—É—é –Ω–æ–≤–æ—Å—Ç–Ω—É—é —Å—Ç–∞—Ç—å—é —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∏—Å–∫–æ–≤ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π:

–ó–ê–ì–û–õ–û–í–û–ö: {title}

–°–û–î–ï–†–ñ–ê–ù–ò–ï: {content}

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∞–Ω–∞–ª–∏–∑ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "financial_relevance": 0.0-1.0,  // —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π
    "risk_indicators": ["indicator1", "indicator2"],  // –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∏—Å–∫–æ–≤
    "sentiment_score": -1.0 to 1.0,  // –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π (-1) –¥–æ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π (+1)
    "key_topics": ["topic1", "topic2"],  // –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã
    "market_impact": "low|medium|high",  // –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä—ã–Ω–æ–∫
    "recommendations": "–∫—Ä–∞—Ç–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏",
    "ai_confidence": 0.0-1.0  // —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∞–Ω–∞–ª–∏–∑–µ
}}

–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
"""
            
            start_time = datetime.utcnow()
            
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º—É –∞–Ω–∞–ª–∏–∑—É. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∏—Å–∫–æ–≤ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –û—Ç–≤–µ—á–∞–µ—à—å —Ç–æ–ª—å–∫–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.3
            )
            
            processing_time = int((datetime.utcnow() - start_time).total_seconds() * 1000)
            
            try:
                analysis_result = json.loads(response.choices[0].message.content)
            except json.JSONDecodeError:
                logger.warning("Failed to parse AI analysis JSON, using defaults")
                analysis_result = {
                    "financial_relevance": 0.5,
                    "risk_indicators": [],
                    "sentiment_score": 0.0,
                    "key_topics": [],
                    "market_impact": "low",
                    "recommendations": "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
                    "ai_confidence": 0.3
                }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            analysis = RSSContentAnalysis(
                user_id=user_id,
                rss_feed_id=rss_feed_id,
                financial_relevance=analysis_result.get('financial_relevance', 0.5),
                risk_indicators=analysis_result.get('risk_indicators', []),
                sentiment_score=analysis_result.get('sentiment_score', 0.0),
                key_topics=analysis_result.get('key_topics', []),
                market_impact=analysis_result.get('market_impact', 'low'),
                recommendations=analysis_result.get('recommendations', ''),
                processing_time_ms=processing_time,
                ai_confidence=analysis_result.get('ai_confidence', 0.5)
            )
            
            db.add(analysis)
            db.commit()
            
            logger.info(f"AI analysis completed for RSS article: {title[:50]}...")
            return analysis_result
            
        except Exception as e:
            logger.error(f"Error in RSS content analysis: {e}")
            return None

    @staticmethod
    async def analyze_financial_data_with_rss(exchange_rates: List[Dict], news_articles: List[Dict], 
                                            user_context: Optional[Dict] = None, user_id: Optional[str] = None, db: Optional[Session] = None):
        """–ê–Ω–∞–ª–∏–∑ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö RSS"""
        try:
            if not client:
                logger.warning("OpenAI client not available, using mock analysis")
                return {
                    'summary': '–¢–µ–∫—É—â–∞—è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —Å–∏—Ç—É–∞—Ü–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è —É–º–µ—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é –≤–∞–ª—é—Ç–Ω–æ–≥–æ –∫—É—Ä—Å–∞.',
                    'risk_assessment': 'MEDIUM',
                    'recommendations': [
                        '–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫—É—Ä—Å–∞ –¥–æ–ª–ª–∞—Ä–∞ –°–®–ê',
                        '–î–∏–≤–µ—Ä—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –≤–∞–ª—é—Ç–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏',
                        '–°–ª–µ–¥–∏—Ç—å –∑–∞ —Ä–µ—à–µ–Ω–∏—è–º–∏ –ù–ë –†–ö –ø–æ –±–∞–∑–æ–≤–æ–π —Å—Ç–∞–≤–∫–µ'
                    ],
                    'forecast': '–ù–∞ –±–ª–∏–∂–∞–π—à–∏–µ 30 –¥–Ω–µ–π –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç—Å—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö —Ç–µ–Ω–¥–µ–Ω—Ü–∏–π',
                    'rss_insights': []
                }
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è AI
            rates_text = "\n".join([f"{r['from_currency']}/{r['to_currency']}: {r['rate']}" for r in exchange_rates[:5]])
            news_text = "\n".join([f"- {n['title']}: {n['content'][:200]}..." for n in news_articles[:10]])
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö RSS (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            rss_insights = []
            if user_id and db:
                try:
                    recent_rss_analysis = db.query(RSSContentAnalysis).filter(
                        RSSContentAnalysis.user_id == user_id,
                        RSSContentAnalysis.analysis_date >= datetime.utcnow() - timedelta(days=1)
                    ).order_by(RSSContentAnalysis.financial_relevance.desc()).limit(5).all()
                    
                    for analysis in recent_rss_analysis:
                        rss_insights.append({
                            'topics': analysis.key_topics,
                            'risk_indicators': analysis.risk_indicators,
                            'market_impact': analysis.market_impact,
                            'recommendations': analysis.recommendations,
                            'sentiment': analysis.sentiment_score,
                            'confidence': analysis.ai_confidence
                        })
                except Exception as e:
                    logger.warning(f"Failed to get RSS insights: {e}")
            
            rss_context = ""
            if rss_insights:
                rss_context = f"\n\n–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–ù–°–ê–ô–¢–´ –ò–ó –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨–°–ö–ò–• RSS:\n"
                for i, insight in enumerate(rss_insights[:3], 1):
                    rss_context += f"{i}. –¢–µ–º—ã: {', '.join(insight['topics'][:3])}\n"
                    rss_context += f"   –†–∏—Å–∫–∏: {', '.join(insight['risk_indicators'][:2])}\n"
                    rss_context += f"   –í–ª–∏—è–Ω–∏–µ: {insight['market_impact']}\n"
            
            prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—É—â—É—é —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö:

–ö–£–†–°–´ –í–ê–õ–Æ–¢ (–ù–ë –†–ö):
{rates_text}

–§–ò–ù–ê–ù–°–û–í–´–ï –ù–û–í–û–°–¢–ò:
{news_text}

{rss_context}

–ö–û–ù–¢–ï–ö–°–¢ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{json.dumps(user_context) if user_context else '–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑'}

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∞–Ω–∞–ª–∏–∑ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "summary": "–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏ (–¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤)",
    "risk_assessment": "LOW|MEDIUM|HIGH",
    "recommendations": ["—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è1", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è2", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è3"],
    "forecast": "–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–µ 30 –¥–Ω–µ–π",
    "rss_insights": ["–∏–Ω—Å–∞–π—Ç1", "–∏–Ω—Å–∞–π—Ç2"],
    "market_factors": ["—Ñ–∞–∫—Ç–æ—Ä1", "—Ñ–∞–∫—Ç–æ—Ä2"]
}}

–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —É—á–∏—Ç—ã–≤–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–≥–æ —Ä—ã–Ω–∫–∞.
"""
            
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–º —Ä—ã–Ω–∫–µ. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –≤–∫–ª—é—á–∞—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ RSS –∫–∞–Ω–∞–ª—ã."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1200,
                temperature=0.3
            )
            
            ai_response = response.choices[0].message.content
            
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞
            try:
                analysis = json.loads(ai_response)
            except json.JSONDecodeError:
                # –ï—Å–ª–∏ AI –Ω–µ –≤–µ—Ä–Ω—É–ª JSON, —Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ —Ç–µ–∫—Å—Ç–∞
                analysis = {
                    'summary': ai_response[:200],
                    'risk_assessment': 'MEDIUM',
                    'recommendations': ai_response.split('\n')[-3:],
                    'forecast': '–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑',
                    'rss_insights': [f"–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(rss_insights)} RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"] if rss_insights else [],
                    'market_factors': []
                }
            
            logger.info("Enhanced AI analysis completed successfully")
            return analysis
            
        except Exception as e:
            logger.error(f"Error in enhanced AI analysis: {e}")
            return {
                'summary': '–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ AI –∞–Ω–∞–ª–∏–∑–∞. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.',
                'risk_assessment': 'MEDIUM',
                'recommendations': [
                    '–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤–∞–ª—é—Ç–Ω—ã—Ö –∫—É—Ä—Å–æ–≤',
                    '–î–∏–≤–µ—Ä—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ä–∏—Å–∫–∏',
                    '–ö–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞–º–∏'
                ],
                'forecast': '–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω',
                'rss_insights': [],
                'market_factors': []
            }

    @staticmethod
    async def answer_question_with_rss(question: str, context: Optional[Dict] = None, user_id: Optional[str] = None, db: Optional[Session] = None):
        """–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É—á–µ—Ç–æ–º RSS –¥–∞–Ω–Ω—ã—Ö"""
        try:
            if not client:
                return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, AI –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –í–∞—à –≤–æ–ø—Ä–æ—Å: '{question}' –±—ã–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if not db:
                db = SessionLocal()
            
            try:
                # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∫—É—Ä—Å—ã –≤–∞–ª—é—Ç
                latest_rates = db.query(ExchangeRate).filter(
                    ExchangeRate.date >= datetime.now() - timedelta(days=1)
                ).all()
                
                # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
                latest_news = db.query(NewsArticle).filter(
                    NewsArticle.created_at >= datetime.now() - timedelta(days=1)
                ).limit(5).all()
                
                # RSS –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                rss_context = ""
                if user_id:
                    recent_rss = db.query(RSSContentAnalysis).filter(
                        RSSContentAnalysis.user_id == user_id,
                        RSSContentAnalysis.analysis_date >= datetime.now() - timedelta(days=2)
                    ).order_by(RSSContentAnalysis.financial_relevance.desc()).limit(3).all()
                    
                    if recent_rss:
                        rss_context = "\n\n–î–∞–Ω–Ω—ã–µ –∏–∑ –≤–∞—à–∏—Ö RSS –∫–∞–Ω–∞–ª–æ–≤:\n"
                        for rss in recent_rss:
                            rss_context += f"- –¢–µ–º—ã: {', '.join(rss.key_topics[:3])}\n"
                            rss_context += f"  –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {rss.recommendations[:100]}...\n"
                
                rates_context = "\n".join([f"{r.from_currency}/{r.to_currency}: {r.rate}" for r in latest_rates])
                news_context = "\n".join([f"- {n.title}" for n in latest_news])
                
            finally:
                if 'db' in locals():
                    db.close()
            
            prompt = f"""
–ö–æ–Ω—Ç–µ–∫—Å—Ç:
–ê–ö–¢–£–ê–õ–¨–ù–´–ï –ö–£–†–°–´ –í–ê–õ–Æ–¢:
{rates_context}

–ü–û–°–õ–ï–î–ù–ò–ï –ù–û–í–û–°–¢–ò:
{news_context}

{rss_context}

–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢:
{json.dumps(context) if context else '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–û—Ç–≤–µ—Ç—å –∫–∞–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–º —Ä—ã–Ω–∫–µ.
–ò—Å–ø–æ–ª—å–∑—É–π –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –≤–∫–ª—é—á–∞—è –¥–∞–Ω–Ω—ã–µ –∏–∑ RSS –∫–∞–Ω–∞–ª–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º, –Ω–æ –∫—Ä–∞—Ç–∫–∏–º (–¥–æ 300 —Å–ª–æ–≤).
"""
            
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç Financial AI Dashboard. –û—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –¥–∞—ë—à—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.4
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Error answering question with RSS: {e}")
            return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É –ø–æ–∑–∂–µ. –í–∞—à –≤–æ–ø—Ä–æ—Å –±—ã–ª: '{question}'"

# =============================================================================
# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
# =============================================================================

async def update_user_rss_feeds():
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö RSS –∫–∞–Ω–∞–ª–æ–≤"""
    logger.info("Starting user RSS feeds update...")
    
    try:
        db = SessionLocal()
        try:
            # –ü–æ–ª—É—á–∞–µ–º RSS –∫–∞–Ω–∞–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å
            now = datetime.utcnow()
            feeds_to_update = db.query(UserRSSFeed).filter(
                UserRSSFeed.is_active == True
            ).all()
            
            for feed in feeds_to_update:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å —ç—Ç–æ—Ç –∫–∞–Ω–∞–ª
                    if feed.last_fetched:
                        time_diff = now - feed.last_fetched
                        
                        if feed.fetch_frequency == "hourly" and time_diff < timedelta(hours=1):
                            continue
                        elif feed.fetch_frequency == "daily" and time_diff < timedelta(days=1):
                            continue
                        elif feed.fetch_frequency == "weekly" and time_diff < timedelta(weeks=1):
                            continue
                    
                    logger.info(f"Updating RSS feed: {feed.name}")
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∏–∑ RSS
                    articles = await EnhancedNewsService.fetch_user_rss_feeds(str(feed.user_id), db)
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Å—Ç–∞—Ç—å—é —Å –ø–æ–º–æ—â—å—é AI
                    for article in articles:
                        if article.get('rss_feed_id') == str(feed.id):
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—å—é –≤ –±–∞–∑—É
                            news_article = NewsArticle(
                                title=article['title'],
                                content=article['content'],
                                url=article['url'],
                                published_at=article['published_at'],
                                source=article['source'],
                                category=article['category'],
                                relevance_score=article.get('relevance_score', 0.5)
                            )
                            db.add(news_article)
                            db.flush()
                            
                            # AI –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏
                            if feed.auto_analysis:
                                await EnhancedAIAnalysisService.analyze_rss_content(
                                    article, str(feed.id), str(feed.user_id), db
                                )
                    
                except Exception as e:
                    logger.error(f"Error updating RSS feed {feed.name}: {e}")
                    feed.error_count += 1
                    feed.last_error = str(e)
                    continue
            
            db.commit()
            logger.info("User RSS feeds update completed")
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"Error in user RSS feeds update: {e}")

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
async def update_exchange_rates():
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—É—Ä—Å–æ–≤ –≤–∞–ª—é—Ç"""
    logger.info("Starting scheduled exchange rates update...")
    
    try:
        rates = await NBKExchangeRateService.fetch_exchange_rates()
        
        db = SessionLocal()
        try:
            for rate_data in rates:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π –∫—É—Ä—Å –Ω–∞ —Å–µ–≥–æ–¥–Ω—è
                existing = db.query(ExchangeRate).filter(
                    ExchangeRate.from_currency == rate_data['from_currency'],
                    ExchangeRate.to_currency == rate_data['to_currency'],
                    ExchangeRate.date >= datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                ).first()
                
                if not existing:
                    rate = ExchangeRate(**rate_data)
                    db.add(rate)
            
            db.commit()
            logger.info(f"Successfully updated {len(rates)} exchange rates")
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"Error updating exchange rates: {e}")

async def update_news():
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
    logger.info("Starting scheduled news update...")
    
    try:
        articles = await EnhancedNewsService.fetch_default_news()
        
        db = SessionLocal()
        try:
            for article_data in articles:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –Ω–æ–≤–æ—Å—Ç—å
                existing = db.query(NewsArticle).filter(
                    NewsArticle.title == article_data['title']
                ).first()
                
                if not existing:
                    # –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —á–µ—Ä–µ–∑ AI (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
                    if any(word in article_data['title'].lower() for word in ['—Ä–æ—Å—Ç', '—É–≤–µ–ª–∏—á–µ–Ω–∏–µ', '—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å']):
                        sentiment = 'positive'
                    elif any(word in article_data['title'].lower() for word in ['–ø–∞–¥–µ–Ω–∏–µ', '—Å–Ω–∏–∂–µ–Ω–∏–µ', '–∫—Ä–∏–∑–∏—Å']):
                        sentiment = 'negative'
                    else:
                        sentiment = 'neutral'
                    
                    article_data['sentiment'] = sentiment
                    article_data['risk_level'] = 'medium'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    
                    news = NewsArticle(**article_data)
                    db.add(news)
            
            db.commit()
            logger.info(f"Successfully updated {len(articles)} news articles")
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"Error updating news: {e}")

async def generate_daily_report():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å RSS –¥–∞–Ω–Ω—ã–º–∏"""
    logger.info("Starting daily report generation...")
    
    try:
        db = SessionLocal()
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            
            rates = db.query(ExchangeRate).filter(
                ExchangeRate.date >= today
            ).all()
            
            news = db.query(NewsArticle).filter(
                NewsArticle.created_at >= today
            ).all()
            
            # AI –∞–Ω–∞–ª–∏–∑ —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö RSS –¥–∞–Ω–Ω—ã—Ö
            analysis = await EnhancedAIAnalysisService.analyze_financial_data_with_rss(
                [{'from_currency': r.from_currency, 'to_currency': r.to_currency, 'rate': r.rate} for r in rates],
                [{'title': n.title, 'content': n.content or ''} for n in news],
                db=db
            )
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–∑ RSS –∞–Ω–∞–ª–∏–∑–∞
            rss_insights = db.query(RSSContentAnalysis).filter(
                RSSContentAnalysis.analysis_date >= today
            ).order_by(RSSContentAnalysis.financial_relevance.desc()).limit(10).all()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
            rss_section = ""
            if rss_insights:
                rss_section = f"""
## üì° –ê–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö RSS –∫–∞–Ω–∞–ª–æ–≤
{chr(10).join([f"- **{', '.join(insight.key_topics[:2])}**: {insight.recommendations[:100]}..." for insight in rss_insights[:5]])}

**–û–±—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:** {sum([insight.sentiment_score for insight in rss_insights]) / len(rss_insights):.2f}
"""
            
            report_content = f"""
# –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç
**–î–∞—Ç–∞:** {datetime.now().strftime('%d.%m.%Y')}

## üìä –ö—É—Ä—Å—ã –≤–∞–ª—é—Ç (–ù–ë –†–ö)
{chr(10).join([f"- {r.from_currency}/{r.to_currency}: {r.rate:.2f}" for r in rates])}

## üìà –ê–Ω–∞–ª–∏–∑ —Å–∏—Ç—É–∞—Ü–∏–∏
{analysis['summary']}

**–£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞:** {analysis['risk_assessment']}

## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
{chr(10).join([f"- {rec}" for rec in analysis['recommendations']])}

{rss_section}

## üîÆ –ü—Ä–æ–≥–Ω–æ–∑
{analysis['forecast']}

## üì∞ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
{chr(10).join([f"- {n.title}" for n in news[:5]])}

---
*–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∏—Å—Ç–µ–º–æ–π Financial AI Dashboard —Å –∞–Ω–∞–ª–∏–∑–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö RSS –∫–∞–Ω–∞–ª–æ–≤*
"""

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
            rss_feeds_data = [str(insight.rss_feed_id) for insight in rss_insights]
            
            report = Report(
                name=f"–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á–µ—Ç {datetime.now().strftime('%d.%m.%Y')}",
                report_type="daily_summary",
                content=report_content,
                rss_feeds_included=rss_feeds_data
            )
            db.add(report)
            db.commit()
            
            logger.info("Enhanced daily report generated successfully")
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"Error generating daily report: {e}")

# –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á
def schedule_tasks():
    """–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á"""
    schedule.every().day.at("08:00").do(lambda: asyncio.run(update_exchange_rates()))
    schedule.every().hour.do(lambda: asyncio.run(update_news()))
    schedule.every().hour.do(lambda: asyncio.run(update_user_rss_feeds()))  # –ù–û–í–û–ï
    schedule.every().day.at("09:00").do(lambda: asyncio.run(generate_daily_report()))
    
    logger.info("Enhanced scheduled tasks configured")

def run_scheduler():
    """–ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    while True:
        schedule.run_pending()
        time.sleep(60)

# –ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
schedule_tasks()
scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
scheduler_thread.start()

# =============================================================================
# API Routes (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∏ –Ω–æ–≤—ã–µ)
# =============================================================================

@app.get("/", response_class=HTMLResponse)
async def read_root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return FileResponse("static/index.html")

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "database": "connected",
            "nbk_api": "available",
            "ai_service": "available" if client else "unavailable",
            "news_service": "available",
            "rss_feeds": "available"
        },
        "version": "2.1.0"
    }

# =============================================================================
# RSS Feed Management API (–ù–û–í–´–ï ENDPOINTS)
# =============================================================================

@app.get("/api/rss/feeds", response_model=List[RSSFeedResponse])
async def get_user_rss_feeds(current_user: User = Depends(get_current_user), db: Session = Depends(get_db)):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ RSS –∫–∞–Ω–∞–ª–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        feeds = db.query(UserRSSFeed).filter(UserRSSFeed.user_id == current_user.id).all()
        
        return [
            RSSFeedResponse(
                id=str(feed.id),
                name=feed.name,
                url=feed.url,
                description=feed.description or "",
                category=feed.category,
                priority=feed.priority,
                is_active=feed.is_active,
                auto_analysis=feed.auto_analysis,
                keywords=feed.keywords or [],
                last_fetched=feed.last_fetched,
                fetch_frequency=feed.fetch_frequency,
                error_count=feed.error_count,
                articles_count=feed.articles_count,
                created_at=feed.created_at
            )
            for feed in feeds
        ]
        
    except Exception as e:
        logger.error(f"Error getting RSS feeds: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RSS –∫–∞–Ω–∞–ª–æ–≤")

@app.post("/api/rss/feeds", response_model=RSSFeedResponse)
async def create_rss_feed(
    feed_data: RSSFeedCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ RSS –∫–∞–Ω–∞–ª–∞"""
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è RSS URL
        if not validate_rss_url(str(feed_data.url)):
            raise HTTPException(status_code=400, detail="–ù–µ–¥–æ—Å—Ç—É–ø–Ω—ã–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π RSS URL")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–∫–∏
        existing_feeds_count = db.query(UserRSSFeed).filter(UserRSSFeed.user_id == current_user.id).count()
        
        max_feeds = 5 if current_user.subscription_plan == "starter" else 20 if current_user.subscription_plan == "professional" else 100
        
        if existing_feeds_count >= max_feeds:
            raise HTTPException(
                status_code=400, 
                detail=f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç RSS –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –ø–ª–∞–Ω–∞ {current_user.subscription_plan}: {max_feeds}"
            )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ RSS –∫–∞–Ω–∞–ª–∞
        rss_feed = UserRSSFeed(
            user_id=current_user.id,
            name=feed_data.name,
            url=str(feed_data.url),
            description=feed_data.description,
            category=feed_data.category,
            priority=feed_data.priority,
            auto_analysis=feed_data.auto_analysis,
            keywords=feed_data.keywords,
            fetch_frequency=feed_data.fetch_frequency
        )
        
        db.add(rss_feed)
        db.commit()
        db.refresh(rss_feed)
        
        # –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        try:
            await EnhancedNewsService.fetch_user_rss_feeds(str(current_user.id), db)
        except Exception as e:
            logger.warning(f"Initial RSS fetch failed: {e}")
        
        return RSSFeedResponse(
            id=str(rss_feed.id),
            name=rss_feed.name,
            url=rss_feed.url,
            description=rss_feed.description or "",
            category=rss_feed.category,
            priority=rss_feed.priority,
            is_active=rss_feed.is_active,
            auto_analysis=rss_feed.auto_analysis,
            keywords=rss_feed.keywords or [],
            last_fetched=rss_feed.last_fetched,
            fetch_frequency=rss_feed.fetch_frequency,
            error_count=rss_feed.error_count,
            articles_count=rss_feed.articles_count,
            created_at=rss_feed.created_at
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating RSS feed: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è RSS –∫–∞–Ω–∞–ª–∞")

@app.put("/api/rss/feeds/{feed_id}", response_model=RSSFeedResponse)
async def update_rss_feed(
    feed_id: str,
    feed_data: RSSFeedUpdate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ RSS –∫–∞–Ω–∞–ª–∞"""
    try:
        feed = db.query(UserRSSFeed).filter(
            UserRSSFeed.id == feed_id,
            UserRSSFeed.user_id == current_user.id
        ).first()
        
        if not feed:
            raise HTTPException(status_code=404, detail="RSS –∫–∞–Ω–∞–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª—è
        update_data = feed_data.dict(exclude_unset=True)
        for field, value in update_data.items():
            setattr(feed, field, value)
        
        feed.updated_at = datetime.utcnow()
        db.commit()
        db.refresh(feed)
        
        return RSSFeedResponse(
            id=str(feed.id),
            name=feed.name,
            url=feed.url,
            description=feed.description or "",
            category=feed.category,
            priority=feed.priority,
            is_active=feed.is_active,
            auto_analysis=feed.auto_analysis,
            keywords=feed.keywords or [],
            last_fetched=feed.last_fetched,
            fetch_frequency=feed.fetch_frequency,
            error_count=feed.error_count,
            articles_count=feed.articles_count,
            created_at=feed.created_at
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating RSS feed: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è RSS –∫–∞–Ω–∞–ª–∞")

@app.delete("/api/rss/feeds/{feed_id}")
async def delete_rss_feed(
    feed_id: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–£–¥–∞–ª–µ–Ω–∏–µ RSS –∫–∞–Ω–∞–ª–∞"""
    try:
        feed = db.query(UserRSSFeed).filter(
            UserRSSFeed.id == feed_id,
            UserRSSFeed.user_id == current_user.id
        ).first()
        
        if not feed:
            raise HTTPException(status_code=404, detail="RSS –∫–∞–Ω–∞–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –£–¥–∞–ª—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã
        db.query(RSSContentAnalysis).filter(
            RSSContentAnalysis.rss_feed_id == feed_id
        ).delete()
        
        # –£–¥–∞–ª—è–µ–º —Å–∞–º –∫–∞–Ω–∞–ª
        db.delete(feed)
        db.commit()
        
        return {"message": "RSS –∫–∞–Ω–∞–ª —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting RSS feed: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è RSS –∫–∞–Ω–∞–ª–∞")

@app.post("/api/rss/feeds/{feed_id}/test")
async def test_rss_feed(
    feed_id: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –∫–∞–Ω–∞–ª–∞"""
    try:
        feed = db.query(UserRSSFeed).filter(
            UserRSSFeed.id == feed_id,
            UserRSSFeed.user_id == current_user.id
        ).first()
        
        if not feed:
            raise HTTPException(status_code=404, detail="RSS –∫–∞–Ω–∞–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –¢–µ—Å—Ç–æ–≤–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        articles = await EnhancedNewsService.fetch_user_rss_feeds(str(current_user.id), db)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–∞—Ç—å–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        feed_articles = [a for a in articles if a.get('rss_feed_id') == feed_id]
        
        return {
            "status": "success",
            "articles_found": len(feed_articles),
            "sample_articles": [
                {
                    "title": article["title"],
                    "url": article["url"],
                    "relevance_score": article.get("relevance_score", 0)
                }
                for article in feed_articles[:3]
            ],
            "last_test": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error testing RSS feed: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è RSS –∫–∞–Ω–∞–ª–∞")

@app.get("/api/rss/analysis")
async def get_rss_analysis(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db),
    days: int = 7
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ RSS –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    try:
        start_date = datetime.utcnow() - timedelta(days=days)
        
        analyses = db.query(RSSContentAnalysis).filter(
            RSSContentAnalysis.user_id == current_user.id,
            RSSContentAnalysis.analysis_date >= start_date
        ).order_by(RSSContentAnalysis.financial_relevance.desc()).limit(50).all()
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_analyses = len(analyses)
        avg_relevance = sum([a.financial_relevance for a in analyses]) / max(total_analyses, 1)
        avg_sentiment = sum([a.sentiment_score for a in analyses]) / max(total_analyses, 1)
        
        # –¢–æ–ø —Ç–µ–º—ã
        all_topics = []
        for analysis in analyses:
            all_topics.extend(analysis.key_topics or [])
        
        from collections import Counter
        top_topics = Counter(all_topics).most_common(10)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤–ª–∏—è–Ω–∏—é –Ω–∞ —Ä—ã–Ω–æ–∫
        impact_distribution = Counter([a.market_impact for a in analyses])
        
        return {
            "summary": {
                "total_analyses": total_analyses,
                "average_relevance": round(avg_relevance, 2),
                "average_sentiment": round(avg_sentiment, 2),
                "period_days": days
            },
            "top_topics": [{"topic": topic, "count": count} for topic, count in top_topics],
            "market_impact_distribution": dict(impact_distribution),
            "recent_analyses": [
                {
                    "id": str(analysis.id),
                    "analysis_date": analysis.analysis_date.isoformat(),
                    "financial_relevance": analysis.financial_relevance,
                    "sentiment_score": analysis.sentiment_score,
                    "key_topics": analysis.key_topics[:3],
                    "market_impact": analysis.market_impact,
                    "recommendations": analysis.recommendations[:100] + "..." if len(analysis.recommendations) > 100 else analysis.recommendations,
                    "ai_confidence": analysis.ai_confidence
                }
                for analysis in analyses[:10]
            ]
        }
        
    except Exception as e:
        logger.error(f"Error getting RSS analysis: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ RSS")

# =============================================================================
# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –æ—Å–Ω–æ–≤–Ω—ã–µ API Routes
# =============================================================================

@app.post("/api/auth/login")
async def login(user_data: UserLogin, db: Session = Depends(get_db)):
    """–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    user = db.query(User).filter(User.email == user_data.email).first()
    if not user or not verify_password(user_data.password, user.hashed_password):
        raise HTTPException(status_code=401, detail="–ù–µ–≤–µ—Ä–Ω—ã–π email –∏–ª–∏ –ø–∞—Ä–æ–ª—å")
    
    user.last_login = datetime.utcnow()
    db.commit()
    
    access_token = create_access_token(
        data={"sub": user.email},
        expires_delta=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "user": {
            "id": str(user.id),
            "email": user.email,
            "name": user.name,
            "company": user.company,
            "subscription_plan": user.subscription_plan
        }
    }

@app.post("/api/auth/register")
async def register(user_data: UserCreate, db: Session = Depends(get_db)):
    """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    existing_user = db.query(User).filter(User.email == user_data.email).first()
    if existing_user:
        raise HTTPException(status_code=400, detail="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å —Ç–∞–∫–∏–º email —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    
    hashed_password = get_password_hash(user_data.password)
    user = User(
        email=user_data.email,
        name=user_data.name,
        hashed_password=hashed_password,
        company=user_data.company
    )
    db.add(user)
    db.commit()
    db.refresh(user)
    
    access_token = create_access_token(
        data={"sub": user.email},
        expires_delta=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "user": {
            "id": str(user.id),
            "email": user.email,
            "name": user.name,
            "company": user.company,
            "subscription_plan": user.subscription_plan
        }
    }

@app.get("/api/dashboard")
async def get_dashboard_data(current_user: User = Depends(get_current_user), db: Session = Depends(get_db)):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞ —Å RSS –∏–Ω—Å–∞–π—Ç–∞–º–∏"""
    try:
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∫—É—Ä—Å–æ–≤ –≤–∞–ª—é—Ç
        latest_rates = db.query(ExchangeRate).filter(
            ExchangeRate.date >= datetime.now() - timedelta(days=1)
        ).all()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        latest_news = db.query(NewsArticle).filter(
            NewsArticle.created_at >= datetime.now() - timedelta(days=1)
        ).limit(5).all()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —Å—á–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        accounts = db.query(BankAccount).filter(BankAccount.user_id == current_user.id).all()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ RSS —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        rss_feeds_count = db.query(UserRSSFeed).filter(
            UserRSSFeed.user_id == current_user.id,
            UserRSSFeed.is_active == True
        ).count()
        
        recent_rss_analyses = db.query(RSSContentAnalysis).filter(
            RSSContentAnalysis.user_id == current_user.id,
            RSSContentAnalysis.analysis_date >= datetime.now() - timedelta(days=1)
        ).order_by(RSSContentAnalysis.financial_relevance.desc()).limit(3).all()
        
        # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –≤ —Ç–µ–Ω–≥–µ
        total_balance_kzt = 0
        for account in accounts:
            if account.currency == "KZT":
                total_balance_kzt += account.balance
            else:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫—É—Ä—Å—ã
                rate = next((r.rate for r in latest_rates if r.from_currency == account.currency and r.to_currency == "KZT"), 1)
                total_balance_kzt += account.balance * rate
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏
        if total_balance_kzt < 30000000:
            liquidity_status = "CRITICAL"
        elif total_balance_kzt < 100000000:
            liquidity_status = "LOW"
        elif total_balance_kzt > 300000000:
            liquidity_status = "EXCESS"
        else:
            liquidity_status = "ADEQUATE"
        
        return {
            "user": {
                "name": current_user.name,
                "company": current_user.company,
                "subscription_plan": current_user.subscription_plan
            },
            "summary": {
                "total_balance_kzt": total_balance_kzt,
                "liquidity_status": liquidity_status,
                "accounts_count": len(accounts),
                "rss_feeds_count": rss_feeds_count,
                "last_updated": datetime.now().isoformat()
            },
            "exchange_rates": [
                {
                    "from_currency": rate.from_currency,
                    "to_currency": rate.to_currency,
                    "rate": rate.rate,
                    "date": rate.date.isoformat(),
                    "source": rate.source
                }
                for rate in latest_rates
            ],
            "news": [
                {
                    "id": str(news.id),
                    "title": news.title,
                    "content": news.content[:200] + "..." if news.content and len(news.content) > 200 else news.content,
                    "url": news.url,
                    "source": news.source,
                    "sentiment": news.sentiment,
                    "published_at": news.published_at.isoformat() if news.published_at else None
                }
                for news in latest_news
            ],
            "rss_insights": [
                {
                    "key_topics": analysis.key_topics[:3],
                    "market_impact": analysis.market_impact,
                    "sentiment_score": analysis.sentiment_score,
                    "financial_relevance": analysis.financial_relevance,
                    "recommendations": analysis.recommendations[:100] + "..." if len(analysis.recommendations) > 100 else analysis.recommendations
                }
                for analysis in recent_rss_analyses
            ],
            "accounts": [
                {
                    "id": str(account.id),
                    "name": account.name,
                    "bank": account.bank,
                    "balance": account.balance,
                    "currency": account.currency,
                    "account_type": account.account_type
                }
                for account in accounts
            ]
        }
        
    except Exception as e:
        logger.error(f"Error getting dashboard data: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–∞—à–±–æ—Ä–¥–∞")

@app.post("/api/ai/consult")
async def ai_consultation(
    request: AIQuestionRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """AI –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º RSS –¥–∞–Ω–Ω—ã—Ö"""
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_context = {
            "user_id": str(current_user.id),
            "company": current_user.company,
            "subscription_plan": current_user.subscription_plan
        }
        user_context.update(request.context)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI —Å —É—á–µ—Ç–æ–º RSS
        if request.include_rss:
            response = await EnhancedAIAnalysisService.answer_question_with_rss(
                request.question, user_context, str(current_user.id), db
            )
            
            # –ü–æ–ª—É—á–∞–µ–º RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã
            rss_sources = db.query(UserRSSFeed).filter(
                UserRSSFeed.user_id == current_user.id,
                UserRSSFeed.is_active == True
            ).limit(5).all()
            
            rss_sources_info = [{"name": feed.name, "category": feed.category} for feed in rss_sources]
        else:
            response = await EnhancedAIAnalysisService.answer_question_with_rss(
                request.question, user_context
            )
            rss_sources_info = []
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏
        consultation = AIConsultation(
            user_id=current_user.id,
            question=request.question,
            response=response,
            context_data=user_context,
            rss_sources_used=rss_sources_info,
            session_id=str(uuid.uuid4()),
            source=request.source
        )
        db.add(consultation)
        db.commit()
        
        return {
            "id": str(consultation.id),
            "question": request.question,
            "response": response,
            "rss_sources_used": rss_sources_info,
            "include_rss": request.include_rss,
            "created_at": consultation.created_at.isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error in AI consultation: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ AI –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏")

@app.get("/api/reports/latest")
async def get_latest_report(current_user: User = Depends(get_current_user), db: Session = Depends(get_db)):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å RSS –¥–∞–Ω–Ω—ã–º–∏"""
    try:
        latest_report = db.query(Report).filter(
            Report.report_type == "daily_summary"
        ).order_by(Report.generated_at.desc()).first()
        
        if not latest_report:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å
            await generate_daily_report()
            latest_report = db.query(Report).filter(
                Report.report_type == "daily_summary"
            ).order_by(Report.generated_at.desc()).first()
        
        return {
            "id": str(latest_report.id),
            "name": latest_report.name,
            "content": latest_report.content,
            "rss_feeds_included": latest_report.rss_feeds_included or [],
            "generated_at": latest_report.generated_at.isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting latest report: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞")

# =============================================================================
# n8n Webhook Integration (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è)
# =============================================================================

@app.post("/webhook/n8n/chat")
async def n8n_chat_webhook(payload: Dict[str, Any], db: Session = Depends(get_db)):
    """Webhook –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å n8n —á–∞—Ç-–±–æ—Ç–æ–º —Å RSS –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π"""
    try:
        logger.info(f"Received n8n webhook: {payload}")
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ payload
        message = payload.get("message", "")
        user_id = payload.get("user_id")
        session_id = payload.get("session_id", str(uuid.uuid4()))
        webhook_url = payload.get("webhook_url")  # URL –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞ –æ–±—Ä–∞—Ç–Ω–æ –≤ n8n
        include_rss = payload.get("include_rss", True)  # –ù–û–í–û–ï: –≤–∫–ª—é—á–∞—Ç—å RSS –≤ –∞–Ω–∞–ª–∏–∑
        
        if not message:
            raise HTTPException(status_code=400, detail="Message is required")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ webhook
        webhook_log = WebhookLog(
            source="n8n",
            event_type="chat_message",
            payload=payload
        )
        db.add(webhook_log)
        db.commit()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI —Å —É—á–µ—Ç–æ–º RSS
        if include_rss and user_id:
            response = await EnhancedAIAnalysisService.answer_question_with_rss(
                message, {"source": "n8n", "session_id": session_id}, user_id, db
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
            user_feeds = db.query(UserRSSFeed).filter(
                UserRSSFeed.user_id == user_id,
                UserRSSFeed.is_active == True
            ).limit(3).all()
            
            rss_info = [f"{feed.name} ({feed.category})" for feed in user_feeds]
        else:
            response = await EnhancedAIAnalysisService.answer_question_with_rss(
                message, {"source": "n8n", "session_id": session_id}
            )
            rss_info = []
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏
        consultation = AIConsultation(
            user_id=user_id if user_id else None,
            question=message,
            response=response,
            context_data={"source": "n8n", "session_id": session_id, "include_rss": include_rss},
            rss_sources_used=rss_info,
            session_id=session_id,
            source="n8n"
        )
        db.add(consultation)
        db.commit()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞
        response_data = {
            "response": response,
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "source": "financial_ai_dashboard",
            "rss_sources_used": rss_info,
            "include_rss": include_rss
        }
        
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω webhook_url, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ –≤ n8n
        if webhook_url:
            try:
                async with httpx.AsyncClient() as client:
                    await client.post(webhook_url, json=response_data)
                logger.info(f"Response sent back to n8n webhook: {webhook_url}")
            except Exception as e:
                logger.error(f"Error sending response to n8n webhook: {e}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–≥ —Å –æ—Ç–≤–µ—Ç–æ–º
        webhook_log.response = response_data
        db.commit()
        
        return response_data
        
    except Exception as e:
        logger.error(f"Error processing n8n webhook: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ webhook")

@app.get("/webhook/n8n/status")
async def n8n_webhook_status():
    """–°—Ç–∞—Ç—É—Å webhook –¥–ª—è n8n —Å RSS –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π"""
    return {
        "status": "active",
        "webhook_url": "/webhook/n8n/chat",
        "supported_methods": ["POST"],
        "required_fields": ["message"],
        "optional_fields": ["user_id", "session_id", "webhook_url", "include_rss"],
        "features": ["rss_analysis", "user_context", "ai_consultation"],
        "version": "2.1.0",
        "timestamp": datetime.now().isoformat()
    }

# =============================================================================
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è)
# =============================================================================

@app.post("/api/init-demo")
async def initialize_demo_data(db: Session = Depends(get_db)):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å RSS –∫–∞–Ω–∞–ª–∞–º–∏"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –¥–µ–º–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
        demo_user = db.query(User).filter(User.email == "demo@finai.kz").first()
        if demo_user:
            return {"message": "Demo data already exists", "demo_email": "demo@finai.kz", "demo_password": "demo123"}
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        demo_user = User(
            email="demo@finai.kz",
            name="Demo CFO",
            hashed_password=get_password_hash("demo123"),
            company="–¢–û–û –ö–∞–∑–∞—Ö–¢—Ä–µ–π–¥ –î–ï–ú–û",
            phone="+7 777 123 4567",
            subscription_plan="professional"
        )
        db.add(demo_user)
        db.flush()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —Å—á–µ—Ç–æ–≤
        demo_accounts = [
            BankAccount(
                user_id=demo_user.id,
                name="–û—Å–Ω–æ–≤–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å—á–µ—Ç",
                bank="Halyk Bank",
                balance=125300000,
                currency="KZT",
                account_type="operational"
            ),
            BankAccount(
                user_id=demo_user.id,
                name="–í–∞–ª—é—Ç–Ω—ã–π —Å—á–µ—Ç USD",
                bank="Kaspi Bank",
                balance=248500,
                currency="USD",
                account_type="currency"
            ),
            BankAccount(
                user_id=demo_user.id,
                name="–†–µ–∑–µ—Ä–≤–Ω—ã–π —Ñ–æ–Ω–¥",
                bank="Forte Bank",
                balance=45200000,
                currency="KZT",
                account_type="reserve"
            )
        ]
        
        for account in demo_accounts:
            db.add(account)
        
        # –ù–û–í–û–ï: –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ RSS –∫–∞–Ω–∞–ª–æ–≤
        demo_rss_feeds = [
            UserRSSFeed(
                user_id=demo_user.id,
                name="Reuters Financial News",
                url="https://feeds.reuters.com/reuters/businessNews",
                description="–ú–∏—Ä–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ç Reuters",
                category="international",
                priority=5,
                auto_analysis=True,
                keywords=["finance", "market", "economy", "oil", "dollar"],
                fetch_frequency="hourly"
            ),
            UserRSSFeed(
                user_id=demo_user.id,
                name="Bloomberg Economics",
                url="https://feeds.bloomberg.com/economics/news.rss",
                description="–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ç Bloomberg",
                category="economics",
                priority=4,
                auto_analysis=True,
                keywords=["central bank", "inflation", "GDP", "policy"],
                fetch_frequency="hourly"
            ),
            UserRSSFeed(
                user_id=demo_user.id,
                name="–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏–µ —Ñ–∏–Ω–∞–Ω—Å—ã",
                url="https://kursiv.kz/feed/",
                description="–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞",
                category="local",
                priority=5,
                auto_analysis=True,
                keywords=["—Ç–µ–Ω–≥–µ", "–ù–ë –†–ö", "—ç–∫–æ–Ω–æ–º–∏–∫–∞", "–±–∞–Ω–∫–∏"],
                fetch_frequency="daily"
            )
        ]
        
        for rss_feed in demo_rss_feeds:
            db.add(rss_feed)
        
        db.commit()
        
        # –ó–∞–ø—É—Å–∫ –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        await update_exchange_rates()
        await update_news()
        await update_user_rss_feeds()  # –ù–û–í–û–ï
        await generate_daily_report()
        
        logger.info("Demo data with RSS feeds initialized successfully")
        
        return {
            "message": "Demo data with RSS feeds initialized successfully",
            "demo_email": "demo@finai.kz",
            "demo_password": "demo123",
            "features": ["NBK exchange rates", "Financial news monitoring", "AI analysis", "n8n integration", "Custom RSS feeds"],
            "demo_rss_feeds": len(demo_rss_feeds)
        }
        
    except Exception as e:
        logger.error(f"Error initializing demo data: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ–º–æ –¥–∞–Ω–Ω—ã—Ö")

if __name__ == "__main__":
    import uvicorn
    logger.info("Starting Enhanced Financial AI Dashboard with RSS support...")
    uvicorn.run(app, host="0.0.0.0", port=8000)